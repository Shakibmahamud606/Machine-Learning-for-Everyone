Spotting bias in machine learning
In the video, we learned about an AI-enabled recruiting software that preferred men because it learned from historic data when more men were hired. When we have models that affect peoples' lives, we need to carefully evaluate them for any discriminatory behavior that can be learned from historic data.

On the right, you have a model that attempts to predict whether someone will default on their loan. You can breakdown the resulting predictions, by different features like demographics and employment status. Play around with these features and see if you can find anything suspicious about who is predicted to default and who isn't.

Which feature(s) should be investigated more for potential bias before deploying the model?

Instructions

Possible Answers
          Race because there is a relatively large difference between African American/ Caucasian proportions in the "No" (32%/68%) and "Yes" (74%/26%) outcomes.
          Job because there are a lot more people who are "skilled" in the "No" outcome than the "Yes" outcome.
          Purpose because loans for "furniture/appliances" are more often predicted to not default.


Answers:Race because there is a relatively large difference between African American/ Caucasian proportions in the "No" (32%/68%) and "Yes" (74%/26%) outcomes.
